ğŸ§  Batch Gradient Descent vs Scikit-Learn Linear Regression

This project compares:

Linear Regression using Scikit-Learn

Custom implementation of Batch Gradient Descent (from scratch)

Performance comparison using RÂ² Score

Training time comparison

Dataset used: Diabetes Dataset

ğŸ“‚ Dataset

We used the built-in dataset from:

scikit-learn

Function:

from sklearn.datasets import load_diabetes
Dataset Details

Total Samples: 442

Input Features: 10

Target: Disease progression measure (continuous value)

ğŸš€ Part 1 â€” Linear Regression (Using Scikit-Learn)

Model used:

from sklearn.linear_model import LinearRegression
Steps:

Load dataset

Train-Test Split (80-20)

Fit model

Predict on test set

Evaluate using RÂ² score

Results

Intercept: 151.8833

RÂ² Score: 0.4399

Scikit-learn uses an optimized closed-form solution (Normal Equation), which gives stable and accurate results.

ğŸ”¥ Part 2 â€” Custom Batch Gradient Descent (From Scratch)

I implemented my own class:

class GDRegressor:

This implementation performs Batch Gradient Descent, meaning:

It uses the entire training dataset

Computes gradient using all samples

Updates weights after each full pass (epoch)

âš™ï¸ Hyperparameters Used

Learning Rate = 0.5

Epochs = 300

ğŸ§® Update Rule Used

For each epoch:

Î¸ = Î¸ âˆ’ Î± Ã— (âˆ‚L / âˆ‚Î¸)
	â€‹
Where:

Î± = learning rate

L = Mean Squared Error (MSE)

Î¸ = parameters (weights + intercept)

ğŸ“Š Results (Custom GD)

Intercept: 152.08

RÂ² Score: 0.4253

Training Time: 0.0304 seconds

ğŸ“ˆ Performance Comparison
Model	RÂ² Score
Scikit-Learn Linear Regression	0.4399
Custom Batch Gradient Descent	0.4253

âœ… The custom implementation performs very close to Scikit-Learn.

This shows:

Correct gradient implementation

Proper convergence

Good learning rate selection


âš¡ Why Batch Gradient Descent Works Here

Because:

Dataset is small (442 samples)

Full dataset computation is fast

Stable convergence with tuned learning rate

For larger datasets, Mini-Batch or Stochastic Gradient Descent would be preferred.


ğŸ§° Technologies Used

Python

NumPy

Scikit-Learn

Jupyter Notebook

ğŸ¯ Conclusion

Building Batch Gradient Descent from scratch strengthened my understanding of:

Optimization

Derivatives in ML

Convergence behavior

Model tuning

This project demonstrates foundational ML understanding beyond just using libraries.
